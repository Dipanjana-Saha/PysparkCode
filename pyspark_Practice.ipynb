{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHJN3Fhzu0Owi7eSz8b0mW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dipanjana-Saha/PysparkCode/blob/main/pyspark_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('diapanjana_pyspark').getOrCreate()"
      ],
      "metadata": {
        "id": "yhKYLGzNxhOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)’’’  \n",
        "You are given an employee dataset containing information about employees and their managers. Each\n",
        "employee has a manager_id that refers to another employee in the same table. Your task to use self-join\n",
        "to find hierarchical relationships between employees, such as finding all employees under specific"
      ],
      "metadata": {
        "id": "RHJaVfhpuZ3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCPgtXRJsEhf",
        "outputId": "a1901cb2-b57c-433e-b6c9-73f231dfa835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+----------+\n",
            "|employee_id|employee_name|manager_id|\n",
            "+-----------+-------------+----------+\n",
            "|          1|        Alice|      NULL|\n",
            "|          2|          Bob|         1|\n",
            "|          3|      Charlie|         1|\n",
            "|          4|        David|         2|\n",
            "|          5|          EVA|         2|\n",
            "|          6|        Frank|         3|\n",
            "|          7|        Grace|         3|\n",
            "+-----------+-------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "data = [(1,\"Alice\",None),(2,\"Bob\",1),(3,\"Charlie\",1),(4,\"David\",2),(5,\"EVA\",2),(6,\"Frank\",3),(7,\"Grace\",3)]\n",
        "columns = [\"employee_id\",\"employee_name\",\"manager_id\"]\n",
        "\n",
        "df=spark.createDataFrame(data,columns)\n",
        "df.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=df.alias(\"emp\").join(df.alias(\"mgr\"), on = col(\"emp.manager_id\")==col(\"mgr.employee_name\"),how =\"left\")\\\n",
        ".select(col(\"mgr.employee_name\").alias(\"Manager\"),\n",
        "    col(\"emp.employee_name\").alias(\"Employee\"))\n",
        "result.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cojw_BzxpGh",
        "outputId": "a0236e5c-27f1-4baf-a7b0-ae8c80fb3197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Manager: string (nullable = true)\n",
            " |-- Employee: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "hierarchy_df = df.alias(\"emp\").join(df.alias(\"mgr\"),\n",
        "                                col(\"emp.manager_id\") == col(\"mgr.employee_id\"), \"left\").select(\n",
        "                                    col(\"mgr.employee_name\").alias(\"Manager\"),\n",
        "                                    col(\"emp.employee_name\").alias(\"Employee\")\n",
        "                                )\n",
        "hierarchy_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J_WIUji3QTR",
        "outputId": "4e0f985e-3408-411a-92aa-5a3fcb2c3943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+\n",
            "|Manager|Employee|\n",
            "+-------+--------+\n",
            "|   NULL|   Alice|\n",
            "|  Alice|     Bob|\n",
            "|  Alice| Charlie|\n",
            "|Charlie|   Frank|\n",
            "|Charlie|   Grace|\n",
            "|    Bob|   David|\n",
            "|    Bob|     EVA|\n",
            "+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)’’’  \n",
        "You are working as a data engineer and the company has a log system where timestamps are recorded\n",
        "for every user action. your manager wants to know how much time each user spends between log in and\n",
        "log out.  \n",
        "  \n",
        "The system generates logs with login_timestamp and logout_timestamp in hours, minutes & seconds.\n",
        "The result should be formatted like \"HH:mm:ss\".’’’  "
      ],
      "metadata": {
        "id": "lW0w4WFj5uC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "data = [(1,\"2025-01-31 08:00:00\",\"2025-01-31 10:30:45\"),(2,\"2025-01-31 09:00:30\",\"2025-01-31 \\\n",
        "12:15:10\"),(3,\"2025-01-31 07:45:00\",\"2025-01-31 09:00:15\")]\n",
        "\n",
        "schema = [\"user_id\",\"login_timestamp\",\"logout_timestamp\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "#df.show()"
      ],
      "metadata": {
        "id": "TWioNiqj50ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.withColumn(\"login_time\",unix_timestamp(\"login_timestamp\"))\n",
        "df=df.withColumn(\"logout_time\",unix_timestamp(\"logout_timestamp\"))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOvrZe_M6JvO",
        "outputId": "60a3ca29-d0ae-4f04-e7f2-c9a6c03236f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------+-----------+\n",
            "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|\n",
            "+-------+-------------------+-------------------+----------+-----------+\n",
            "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738310400| 1738319445|\n",
            "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738314030| 1738325710|\n",
            "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738309500| 1738314015|\n",
            "+-------+-------------------+-------------------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.withColumn(\"DURATION\",col(\"logout_time\")-col(\"login_time\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSozUCk17XCP",
        "outputId": "3fcfe021-e6f1-49a0-e9a5-f7e6d117f65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------+-----------+--------+\n",
            "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|DURATION|\n",
            "+-------+-------------------+-------------------+----------+-----------+--------+\n",
            "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738310400| 1738319445|    9045|\n",
            "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738314030| 1738325710|   11680|\n",
            "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738309500| 1738314015|    4515|\n",
            "+-------+-------------------+-------------------+----------+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.withColumn(\"hours\",(col(\"DURATION\")/3600).cast(\"int\"))\n",
        "df=df.withColumn(\"minutes\",((col(\"DURATION\")%3600)/60).cast(\"int\"))\n",
        "df = df.withColumn(\"seconds\",(col(\"DURATION\")%60).cast(\"int\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QXAatrb9SRl",
        "outputId": "7a4ce3e1-72e5-4720-d271-e6246b7d751a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------+-----------+--------+-----+-------+-------+\n",
            "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|DURATION|hours|minutes|seconds|\n",
            "+-------+-------------------+-------------------+----------+-----------+--------+-----+-------+-------+\n",
            "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738310400| 1738319445|    9045|    2|     30|     45|\n",
            "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738314030| 1738325710|   11680|    3|     14|     40|\n",
            "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738309500| 1738314015|    4515|    1|     15|     15|\n",
            "+-------+-------------------+-------------------+----------+-----------+--------+-----+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)’’’  \n",
        "Given a dataset of monthly sales records with salespeople names and their regions, calculate the month\n",
        "with the highest sales for each region using window functions and the max() function. Ensure that the\n",
        "result includes the region name, month, and sales value. Consider sales fluctuations, and the dataset\n",
        "should contain multiple records for each region to test windowing correctly."
      ],
      "metadata": {
        "id": "iI95uwmU-lRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [ (\"Amit\", \"North\", \"Jan\", 12000), (\"Rajesh\", \"North\", \"Feb\", 15000), (\"Sunita\", \"North\", \"Mar\",\n",
        "11000), (\"Meena\", \"South\", \"Jan\", 17000), (\"Ravi\", \"South\", \"Feb\", 20000), (\"Priya\", \"South\", \"Mar\",\n",
        "18000), (\"Suresh\", \"East\", \"Jan\", 10000), (\"Vishal\", \"East\", \"Feb\", 22000), (\"Akash\", \"East\", \"Mar\", 21000),\n",
        "(\"Anjali\", \"West\", \"Jan\", 15000), (\"Deepak\", \"West\", \"Feb\", 13000), (\"Nidhi\", \"West\", \"Mar\", 17000) ]\n",
        "\n",
        "columns =  [\"Salesperson\", \"Region\", \"Month\", \"Sales\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "RBdU0dp3-onY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e5bb75-e48e-497f-f906-5e791a25ccf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+-----+-----+\n",
            "|Salesperson|Region|Month|Sales|\n",
            "+-----------+------+-----+-----+\n",
            "|       Amit| North|  Jan|12000|\n",
            "|     Rajesh| North|  Feb|15000|\n",
            "|     Sunita| North|  Mar|11000|\n",
            "|      Meena| South|  Jan|17000|\n",
            "|       Ravi| South|  Feb|20000|\n",
            "|      Priya| South|  Mar|18000|\n",
            "|     Suresh|  East|  Jan|10000|\n",
            "|     Vishal|  East|  Feb|22000|\n",
            "|      Akash|  East|  Mar|21000|\n",
            "|     Anjali|  West|  Jan|15000|\n",
            "|     Deepak|  West|  Feb|13000|\n",
            "|      Nidhi|  West|  Mar|17000|\n",
            "+-----------+------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "windowspex = Window.partitionBy(\"Region\").orderBy(col(\"sales\").desc())\n",
        "ranked_df= df.withColumn(\"rank\",row_number().over(windowspex))\n",
        "ranked_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei05zjGtkvXp",
        "outputId": "cb2dbc17-9664-4d0d-9aa9-f710f507353a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+-----+-----+----+\n",
            "|Salesperson|Region|Month|Sales|rank|\n",
            "+-----------+------+-----+-----+----+\n",
            "|     Vishal|  East|  Feb|22000|   1|\n",
            "|      Akash|  East|  Mar|21000|   2|\n",
            "|     Suresh|  East|  Jan|10000|   3|\n",
            "|     Rajesh| North|  Feb|15000|   1|\n",
            "|       Amit| North|  Jan|12000|   2|\n",
            "|     Sunita| North|  Mar|11000|   3|\n",
            "|       Ravi| South|  Feb|20000|   1|\n",
            "|      Priya| South|  Mar|18000|   2|\n",
            "|      Meena| South|  Jan|17000|   3|\n",
            "|      Nidhi|  West|  Mar|17000|   1|\n",
            "|     Anjali|  West|  Jan|15000|   2|\n",
            "|     Deepak|  West|  Feb|13000|   3|\n",
            "+-----------+------+-----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_rank=ranked_df.filter(col(\"rank\")==1).select(\"Region\",\"Sales\",\"Month\")\n",
        "max_rank.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEoLBYjKmIfR",
        "outputId": "66123870-28de-42f5-eaf1-16e6eb36ec9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+\n",
            "|Region|Sales|Month|\n",
            "+------+-----+-----+\n",
            "|  East|22000|  Feb|\n",
            "| North|15000|  Feb|\n",
            "| South|20000|  Feb|\n",
            "|  West|17000|  Mar|\n",
            "+------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ")’’’In PySpark, StructType and StructField are used for defining schema in DataFrames. Consider a\n",
        "scenario where you have employee data stored in a CSV file with the following structure:  \n",
        "Define an explicit schema for this dataset using StructType and StructField.  \n",
        "Load this data into a PySpark DataFrame using the defined schema.  \n",
        "Extract the employees who belong to the \"IT\" department and have a salary greater than 70000.  \n",
        "Split the Address column into two separate columns: City and State.  \n",
        "Save the transformed data into a Parquet file.’’’  "
      ],
      "metadata": {
        "id": "eLxznAx4mqlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "schema = StructType([ StructField(\"Emp_ID\", IntegerType(), True),\n",
        "                    StructField(\"Name\", StringType(), True),\n",
        "                    StructField(\"Age\", IntegerType(), True),\n",
        "                    StructField(\"Salary\", IntegerType(), True),\n",
        "                    StructField(\"Department\", StringType(), True),\n",
        "                    StructField(\"Address\", StringType(), True) ])\n",
        "\n",
        "data = [ (101, \"Rajesh\", 30, 60000, \"IT\", \"Mumbai-Maharashtra\"), (102, \"Priya\", 28, 75000, \"HR\",\n",
        "\"Bengaluru-Karnataka\"), (103, \"Suresh\", 35, 50000, \"Finance\", \"Chennai-Tamil Nadu\"), (104, \"Anjali\", 25,\n",
        "80000, \"IT\", \"Pune-Maharashtra\"), (105, \"Arjun\", 40, 90000, \"Management\", \"Hyderabad-Telangana\") ]\n",
        "\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJSTspGbmqL-",
        "outputId": "bde9883f-4fdf-429f-8d40-8c90fcfdbedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+---+------+----------+-------------------+\n",
            "|Emp_ID|  Name|Age|Salary|Department|            Address|\n",
            "+------+------+---+------+----------+-------------------+\n",
            "|   101|Rajesh| 30| 60000|        IT| Mumbai-Maharashtra|\n",
            "|   102| Priya| 28| 75000|        HR|Bengaluru-Karnataka|\n",
            "|   103|Suresh| 35| 50000|   Finance| Chennai-Tamil Nadu|\n",
            "|   104|Anjali| 25| 80000|        IT|   Pune-Maharashtra|\n",
            "|   105| Arjun| 40| 90000|Management|Hyderabad-Telangana|\n",
            "+------+------+---+------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter= df.filter((col(\"Department\")=='IT') & (col(\"Salary\")>70000))\n",
        "df_filter.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V39D9C7tr7fr",
        "outputId": "3f4626a0-4bb1-4068-9b39-5863dd583046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+---+------+----------+----------------+\n",
            "|Emp_ID|  Name|Age|Salary|Department|         Address|\n",
            "+------+------+---+------+----------+----------------+\n",
            "|   104|Anjali| 25| 80000|        IT|Pune-Maharashtra|\n",
            "+------+------+---+------+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_df=df.withColumn(\"City\",split(col(\"Address\"),\"-\")[0])\\\n",
        ".withColumn(\"State\",split(col(\"Address\"),\"-\")[1])\n",
        "transformed_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V4_JDOssS1K",
        "outputId": "98257170-888b-42b6-f5b5-7dd8b2a97541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+---+------+----------+-------------------+---------+-----------+\n",
            "|Emp_ID|  Name|Age|Salary|Department|            Address|     City|      State|\n",
            "+------+------+---+------+----------+-------------------+---------+-----------+\n",
            "|   101|Rajesh| 30| 60000|        IT| Mumbai-Maharashtra|   Mumbai|Maharashtra|\n",
            "|   102| Priya| 28| 75000|        HR|Bengaluru-Karnataka|Bengaluru|  Karnataka|\n",
            "|   103|Suresh| 35| 50000|   Finance| Chennai-Tamil Nadu|  Chennai| Tamil Nadu|\n",
            "|   104|Anjali| 25| 80000|        IT|   Pune-Maharashtra|     Pune|Maharashtra|\n",
            "|   105| Arjun| 40| 90000|Management|Hyderabad-Telangana|Hyderabad|  Telangana|\n",
            "+------+------+---+------+----------+-------------------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the data in parquest form\n",
        "transformed_df.write.mode(\"overwrite\").parquet(\"/downloads/data.parquet\")"
      ],
      "metadata": {
        "id": "rmGkc-8utUDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7)’’’  \n",
        "You have a dataset of user activities in an e-commerce application, where each row represents an activity\n",
        " performed by a user. The dataset contains duplicate activity entries (based on user and activity type) and\n",
        "you need to remove the duplicates. Furthermore, you want to keep only the most recent record for each\n",
        "user, based on a timestamp column.  \n",
        "  \n",
        "Problem  \n",
        "Remove duplicates based on user_id and activity_type.  \n",
        "Keep only the most recent activity_timestamp for each user and activity type combination."
      ],
      "metadata": {
        "id": "tGPSYm2_t0EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"login\", \"2025-02-01 10:00:00\"),\n",
        "    (1, \"view_product\", \"2025-02-01 10:05:00\"),\n",
        "    (1, \"login\", \"2025-02-01 10:30:00\"),\n",
        "    (2, \"purchase\", \"2025-02-01 11:00:00\"),\n",
        "    (2, \"login\", \"2025-02-01 11:15:00\"),\n",
        "    (2, \"view_product\", \"2025-02-01 11:30:00\"),\n",
        "    (3, \"login\", \"2025-02-01 12:00:00\"),\n",
        "    (3, \"login\", \"2025-02-01 12:05:00\")\n",
        "]\n",
        "\n",
        "columns = [\"user_id\", \"activity_type\", \"activity_timestamp\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()  # Use df.show() instead of display(df) in standard PySpark scripts\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enCoTiczt4wW",
        "outputId": "efbbc31e-4a6e-4172-d856-16ce51130789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+-------------------+\n",
            "|user_id|activity_type| activity_timestamp|\n",
            "+-------+-------------+-------------------+\n",
            "|      1|        login|2025-02-01 10:00:00|\n",
            "|      1| view_product|2025-02-01 10:05:00|\n",
            "|      1|        login|2025-02-01 10:30:00|\n",
            "|      2|     purchase|2025-02-01 11:00:00|\n",
            "|      2|        login|2025-02-01 11:15:00|\n",
            "|      2| view_product|2025-02-01 11:30:00|\n",
            "|      3|        login|2025-02-01 12:00:00|\n",
            "|      3|        login|2025-02-01 12:05:00|\n",
            "+-------+-------------+-------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: long (nullable = true)\n",
            " |-- activity_type: string (nullable = true)\n",
            " |-- activity_timestamp: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_timestamp =  df.withColumn(\"activity_timestamp\",col(\"activity_timestamp\").cast(\"timestamp\"))\n",
        "df_timestamp.show()\n",
        "df_timestamp.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m85poRa2nIS",
        "outputId": "c90a6a44-c1ca-4dc4-ac0a-bddc7e490d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+-------------------+\n",
            "|user_id|activity_type| activity_timestamp|\n",
            "+-------+-------------+-------------------+\n",
            "|      1|        login|2025-02-01 10:00:00|\n",
            "|      1| view_product|2025-02-01 10:05:00|\n",
            "|      1|        login|2025-02-01 10:30:00|\n",
            "|      2|     purchase|2025-02-01 11:00:00|\n",
            "|      2|        login|2025-02-01 11:15:00|\n",
            "|      2| view_product|2025-02-01 11:30:00|\n",
            "|      3|        login|2025-02-01 12:00:00|\n",
            "|      3|        login|2025-02-01 12:05:00|\n",
            "+-------+-------------+-------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: long (nullable = true)\n",
            " |-- activity_type: string (nullable = true)\n",
            " |-- activity_timestamp: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "windows_spec=Window.partitionBy(\"user_id\",\"activity_type\").orderBy(col(\"activity_timestamp\").desc())\n",
        "transformed_df=df_timestamp.withColumn(\"rank\",row_number().over(windows_spec))\n",
        "transformed_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm-Kkpl4bB3W",
        "outputId": "87c0d116-d43d-4452-b505-b73c19e50daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+-------------------+----+\n",
            "|user_id|activity_type| activity_timestamp|rank|\n",
            "+-------+-------------+-------------------+----+\n",
            "|      1|        login|2025-02-01 10:30:00|   1|\n",
            "|      1|        login|2025-02-01 10:00:00|   2|\n",
            "|      1| view_product|2025-02-01 10:05:00|   1|\n",
            "|      2|        login|2025-02-01 11:15:00|   1|\n",
            "|      2|     purchase|2025-02-01 11:00:00|   1|\n",
            "|      2| view_product|2025-02-01 11:30:00|   1|\n",
            "|      3|        login|2025-02-01 12:05:00|   1|\n",
            "|      3|        login|2025-02-01 12:00:00|   2|\n",
            "+-------+-------------+-------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final=transformed_df.filter(col(\"rank\")==1).drop(\"rank\")\n",
        "df_final.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuH_4OI8d-ID",
        "outputId": "be2bb982-c729-4801-8c81-65cd0267b058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+-------------------+\n",
            "|user_id|activity_type| activity_timestamp|\n",
            "+-------+-------------+-------------------+\n",
            "|      1|        login|2025-02-01 10:30:00|\n",
            "|      1| view_product|2025-02-01 10:05:00|\n",
            "|      2|        login|2025-02-01 11:15:00|\n",
            "|      2|     purchase|2025-02-01 11:00:00|\n",
            "|      2| view_product|2025-02-01 11:30:00|\n",
            "|      3|        login|2025-02-01 12:05:00|\n",
            "+-------+-------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8)’’’You are working as a Data Engineer for a company. The sales team has provided you with a dataset\n",
        "containing sales information. However, the data has some missing values that need to be addressed\n",
        "before processing. You are required to perform the following tasks:  \n",
        "1. Load the following sample dataset into a PySpark DataFrame:  \n",
        "2. Perform the following operations:  \n",
        "    a. Replace all NULL values in the Quantity column with 0.  \n",
        "    b. Replace all NULL values in the Price column with the average price of the existing data.  \n",
        "    c. Drop rows where the Product column is NULL.  \n",
        "    d. Fill missing Sales_Date with a default value of ’2025-01-01’.  \n",
        "    e. Drop rows where all columns are NULL.’’’"
      ],
      "metadata": {
        "id": "f4tFNUSHeglf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "data = [ (1, \"Laptop\", 10, 50000, \"North\", \"2025-01-01\"), (2, \"Mobile\", None, 15000, \"South\", None), (3,\n",
        "\"Tablet\", 20, None, \"West\", \"2025-01-03\"), (4, \"Desktop\", 15, 30000, None, \"2025-01-04\"), (5, None,\n",
        "None, None, \"East\", \"2025-01-05\") ]\n",
        "columns = [\"Sales_ID\", \"Product\", \"Quantity\", \"Price\", \"Region\", \"Sales_Date\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhNXL4FtejZ-",
        "outputId": "021adfa3-ebc2-49e0-ef2a-6686f9e8a607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+--------+-----+------+----------+\n",
            "|Sales_ID|Product|Quantity|Price|Region|Sales_Date|\n",
            "+--------+-------+--------+-----+------+----------+\n",
            "|       1| Laptop|      10|50000| North|2025-01-01|\n",
            "|       2| Mobile|    NULL|15000| South|      NULL|\n",
            "|       3| Tablet|      20| NULL|  West|2025-01-03|\n",
            "|       4|Desktop|      15|30000|  NULL|2025-01-04|\n",
            "|       5|   NULL|    NULL| NULL|  East|2025-01-05|\n",
            "+--------+-------+--------+-----+------+----------+\n",
            "\n",
            "root\n",
            " |-- Sales_ID: long (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Quantity: long (nullable = true)\n",
            " |-- Price: long (nullable = true)\n",
            " |-- Region: string (nullable = true)\n",
            " |-- Sales_Date: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4oKoCmLtYbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WHMZ8NSytYXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)You have been given a large dataset containing employee salary details. Your goal is to optimize a PySpark job that performs a groupBy operation while minimizing the shuffle.\n",
        "\n",
        "Task:\n",
        "Write a PySpark job to calculate the total salary per department.\n",
        "Optimize the job to reduce shuffle while performing the groupBy operation.\n",
        "Explain why your optimization reduces shuffle and improves performance.\n"
      ],
      "metadata": {
        "id": "2dn-jA3btY6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach:\n",
        "To minimize shuffle during a groupBy operation, we should:\n",
        "Use repartition() efficiently to avoid unnecessary partitions.\n",
        "Use reduceByKey() instead of groupByKey(), as it performs local aggregation before shuffling.\n",
        "If working with a DataFrame, use partitionBy() while writing output."
      ],
      "metadata": {
        "id": "k5Da5M6HxMm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "data = [ (101, \"Rahul\", \"IT\", 90000), (102, \"Sita\", \"HR\", 75000), (103, \"Vikram\", \"IT\", 85000), (104, \"Priya\", \"HR\", 72000), (105, \"Anjali\", \"IT\", 88000), (106, \"Manish\", \"Sales\", 67000), (107, \"Neha\", \"Sales\", 70000) ]\n",
        "\n",
        "columns = [\"emp_id\", \"name\", \"dept\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCBzG2amthh7",
        "outputId": "6cd70be2-f9d3-4f63-8011-2be3ef4c976b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----+------+\n",
            "|emp_id|  name| dept|salary|\n",
            "+------+------+-----+------+\n",
            "|   101| Rahul|   IT| 90000|\n",
            "|   102|  Sita|   HR| 75000|\n",
            "|   103|Vikram|   IT| 85000|\n",
            "|   104| Priya|   HR| 72000|\n",
            "|   105|Anjali|   IT| 88000|\n",
            "|   106|Manish|Sales| 67000|\n",
            "|   107|  Neha|Sales| 70000|\n",
            "+------+------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optimzied approach\n",
        "df1=df.repartition(\"dept\").groupBy(\"dept\").agg(sum(\"salary\").alias(\"Total Salary\"))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCAG9U7ixvhw",
        "outputId": "02b20792-6abf-49f4-e0d8-f5248baadfbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "| dept|Total Salary|\n",
            "+-----+------------+\n",
            "|   HR|      147000|\n",
            "|   IT|      263000|\n",
            "|Sales|      137000|\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#anothre approach by using rdd\n",
        "rdd=df.rdd.map(lambda x:(x[2],x[3]))\n",
        "optimized_df=rdd.reduceByKey(lambda x,y:x+y).toDF([\"dept\",\"TotalSlary\"])\n",
        "optimized_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDgM-BHpz0kC",
        "outputId": "a51b19bb-812f-44ea-fdb0-675552053ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "| dept|TotalSlary|\n",
            "+-----+----------+\n",
            "|   IT|    263000|\n",
            "|   HR|    147000|\n",
            "|Sales|    137000|\n",
            "+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)You are given a dataset containing sales transactions for different products. The dataset consists of columns for product_id, product_name, and sales_amount. Your task is to calculate the total sales for each product and filter out only the products with total sales greater than $1000."
      ],
      "metadata": {
        "id": "jrq-JHiR069Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "data = [(1, \"Product A\", 500), (1, \"Product A\", 600),(2, \"Product B\", 300), (3, \"Product C\", 1200),(3, \"Product C\", 800), (4, \"Product D\", 900)]\n",
        "\n",
        "columns = [\"product_id\", \"product_name\", \"sales_amount\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDTscTtZz4Cb",
        "outputId": "196bf9aa-4742-4052-f176-197213a07515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+------------+\n",
            "|product_id|product_name|sales_amount|\n",
            "+----------+------------+------------+\n",
            "|         1|   Product A|         500|\n",
            "|         1|   Product A|         600|\n",
            "|         2|   Product B|         300|\n",
            "|         3|   Product C|        1200|\n",
            "|         3|   Product C|         800|\n",
            "|         4|   Product D|         900|\n",
            "+----------+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1=df.groupBy(\"product_name\").agg(sum(\"sales_amount\").alias(\"totalSales\"))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lwJ1r9x1OqI",
        "outputId": "fc510034-c52d-4e09-e791-af81d09d1e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+\n",
            "|product_name|totalSales|\n",
            "+------------+----------+\n",
            "|   Product A|      1100|\n",
            "|   Product B|       300|\n",
            "|   Product C|      2000|\n",
            "|   Product D|       900|\n",
            "+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df=df1.filter(col(\"totalSales\")>1000)\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI7Rwvm612-9",
        "outputId": "8c80d111-b425-4586-f375-afe19826c0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+\n",
            "|product_name|totalSales|\n",
            "+------------+----------+\n",
            "|   Product A|      1100|\n",
            "|   Product C|      2000|\n",
            "+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)You are working as a Data Engineer at a fintech company. Your team is working on integrating two datasets:\n",
        "Customer Transactions Data (transactions_df) - Contains customer transactions with columns: customer_id, txn_id, amount, and txn_date.\n",
        "Customer Profile Data (profile_df) - Contains customer information with columns: customer_id, name, age, and txn_id (latest transaction ID for reference).\n",
        "The requirement is to merge these two DataFrames on customer_id while keeping track of:\n",
        "Conflicting column names (txn_id) should be renamed properly.\n",
        "If a customer exists in profile_df but not in transactions_df, the row should still be present with NULL values for transaction-related columns.\n",
        "Your task is to write an optimized PySpark code to achieve this."
      ],
      "metadata": {
        "id": "lTM3jhEl2Ng2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_data = [ (101, \"T001\", 500, \"2024-08-10\"), (102, \"T002\", 1200, \"2024-08-09\"), (103, \"T003\", 300, \"2024-08-08\"), (104, \"T004\", 450, \"2024-08-07\"), ]\n",
        "\n",
        "profile_data = [ (101, \"John\", 30, \"T001\"), (102, \"Emma\", 27, \"T005\"), (103, \"Alex\", 35, \"T003\"), (105, \"Sam\", 40, \"T006\"), ]\n",
        "\n",
        "columns1 = [\"customer_id\", \"txn_id\", \"amount\", \"txn_date\"]\n",
        "columns2 = [\"customer_id\", \"name\", \"age\", \"txn_id\"]\n",
        "\n",
        "transaction_df = spark.createDataFrame(transactions_data,columns1)\n",
        "profile_df = spark.createDataFrame(profile_data,columns2)\n",
        "\n",
        "transaction_df.show()\n",
        "profile_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z6aZfm53ZVg",
        "outputId": "76c69cc7-cf19-402d-daa1-151dc2cc7fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+------+----------+\n",
            "|customer_id|txn_id|amount|  txn_date|\n",
            "+-----------+------+------+----------+\n",
            "|        101|  T001|   500|2024-08-10|\n",
            "|        102|  T002|  1200|2024-08-09|\n",
            "|        103|  T003|   300|2024-08-08|\n",
            "|        104|  T004|   450|2024-08-07|\n",
            "+-----------+------+------+----------+\n",
            "\n",
            "+-----------+----+---+------+\n",
            "|customer_id|name|age|txn_id|\n",
            "+-----------+----+---+------+\n",
            "|        101|John| 30|  T001|\n",
            "|        102|Emma| 27|  T005|\n",
            "|        103|Alex| 35|  T003|\n",
            "|        105| Sam| 40|  T006|\n",
            "+-----------+----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "profile_df=profile_df.withColumnRenamed(\"txn_id\",\"last_txn_id\")\n",
        "merge_df=profile_df.join(transaction_df,on=\"customer_id\",how=\"outer\")\n",
        "merge_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F10PYIkW5y8K",
        "outputId": "1334a59f-13bc-4f0e-db44-b061adef32ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----+----+-----------+------+------+----------+\n",
            "|customer_id|name| age|last_txn_id|txn_id|amount|  txn_date|\n",
            "+-----------+----+----+-----------+------+------+----------+\n",
            "|        101|John|  30|       T001|  T001|   500|2024-08-10|\n",
            "|        102|Emma|  27|       T005|  T002|  1200|2024-08-09|\n",
            "|        103|Alex|  35|       T003|  T003|   300|2024-08-08|\n",
            "|        104|NULL|NULL|       NULL|  T004|   450|2024-08-07|\n",
            "|        105| Sam|  40|       T006|  NULL|  NULL|      NULL|\n",
            "+-----------+----+----+-----------+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)You are given a sales transaction dataset containing sales details. Your task is to write a PySpark transformation to:\n",
        "Calculate the total sales per product.\n",
        "Filter out products with total sales greater than $1000."
      ],
      "metadata": {
        "id": "jrthf_1dXtHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "data = [ (1, \"P001\", \"Laptop\", 2, 600), (2, \"P002\", \"Phone\", 3, 300), (3, \"P001\", \"Laptop\", 1, 600), (4, \"P003\", \"Tablet\", 5, 150), (5, \"P002\", \"Phone\", 1, 300) ]\n",
        "\n",
        "columns = [\"transaction_id\", \"product_id\", \"product_name\", \"quantity\", \"price_per_unit\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP8VsYupX7FL",
        "outputId": "481b6556-4672-4e69-f5b7-066114bfe833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----------+------------+--------+--------------+\n",
            "|transaction_id|product_id|product_name|quantity|price_per_unit|\n",
            "+--------------+----------+------------+--------+--------------+\n",
            "|             1|      P001|      Laptop|       2|           600|\n",
            "|             2|      P002|       Phone|       3|           300|\n",
            "|             3|      P001|      Laptop|       1|           600|\n",
            "|             4|      P003|      Tablet|       5|           150|\n",
            "|             5|      P002|       Phone|       1|           300|\n",
            "+--------------+----------+------------+--------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total=df.withColumn(\"totalPrice\",col(\"quantity\")*col(\"price_per_unit\"))\n",
        "total.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH53evowYVJ5",
        "outputId": "a7d0af86-9ee6-4914-9f4f-c2d87a72c992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----------+------------+--------+--------------+----------+\n",
            "|transaction_id|product_id|product_name|quantity|price_per_unit|totalPrice|\n",
            "+--------------+----------+------------+--------+--------------+----------+\n",
            "|             1|      P001|      Laptop|       2|           600|      1200|\n",
            "|             2|      P002|       Phone|       3|           300|       900|\n",
            "|             3|      P001|      Laptop|       1|           600|       600|\n",
            "|             4|      P003|      Tablet|       5|           150|       750|\n",
            "|             5|      P002|       Phone|       1|           300|       300|\n",
            "+--------------+----------+------------+--------+--------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total.groupBy(\"product_id\",\"product_name\").agg(sum(\"totalPrice\").alias(\"final_total\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LRdwInrY9BF",
        "outputId": "f58df980-541e-4b69-bccc-f2224fb0a4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----------+\n",
            "|product_id|product_name|final_total|\n",
            "+----------+------------+-----------+\n",
            "|      P002|       Phone|       1200|\n",
            "|      P001|      Laptop|       1800|\n",
            "|      P003|      Tablet|        750|\n",
            "+----------+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)You are working with large datasets in PySpark and need to join two DataFrames. However, one of the tables has highly skewed data, causing performance issues due to data shuffling. How would you optimize this join using salting techniques?\n",
        "You are given the following sample datasets:\n",
        "sales_df (Fact Table - Large Dataset, Highly Skewed on store_id)\n",
        "Your task is to perform an optimized join between sales_df and store_df on store_id, ensuring that the skewness does not degrade performance."
      ],
      "metadata": {
        "id": "fOmNCWjWacTr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lcYAdOXearnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6)Imagine you're analyzing the monthly sales performance of a company across different regions. You want to calculate:\n",
        "The cumulative sales for each region over months.\n",
        "The rank of each month based on sales within the same region.\n"
      ],
      "metadata": {
        "id": "4p8_-HpSefz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import *\n",
        "\n",
        "data = [ (\"East\", \"Jan\", 200), (\"East\", \"Feb\", 200), (\"East\", \"Mar\", 150), (\"West\", \"Jan\", 400), (\"West\", \"Feb\", 350), (\"West\", \"Mar\", 450) ]\n",
        "\n",
        "columns = [\"Region\", \"Month\", \"Sales\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLidNAXVehGi",
        "outputId": "51d14270-b7b6-4889-80be-ed6eb316bb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+\n",
            "|Region|Month|Sales|\n",
            "+------+-----+-----+\n",
            "|  East|  Jan|  200|\n",
            "|  East|  Feb|  200|\n",
            "|  East|  Mar|  150|\n",
            "|  West|  Jan|  400|\n",
            "|  West|  Feb|  350|\n",
            "|  West|  Mar|  450|\n",
            "+------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_cum=Window.partitionBy(\"Region\").orderBy(\"Month\")\n",
        "\n",
        "window_spec=Window.partitionBy(\"Region\").orderBy(col(\"Sales\").desc())\n",
        "\n",
        "df=df.withColumn(\"cumulative\",sum(\"Sales\").over(window_cum))\\\n",
        ".withColumn(\"rank\",rank().over(window_spec))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqrZvBTPgWT7",
        "outputId": "9f695769-26bc-4b4c-c02d-83ae4d0b50e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+----------+----+\n",
            "|Region|Month|Sales|cumulative|rank|\n",
            "+------+-----+-----+----------+----+\n",
            "|  East|  Feb|  200|       200|   1|\n",
            "|  East|  Jan|  200|       400|   1|\n",
            "|  East|  Mar|  150|       550|   3|\n",
            "|  West|  Mar|  450|      1200|   1|\n",
            "|  West|  Jan|  400|       750|   2|\n",
            "|  West|  Feb|  350|       350|   3|\n",
            "+------+-----+-----+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_cum=Window.partitionBy(\"Region\").orderBy(\"Month\")\n",
        "\n",
        "window_spec=Window.partitionBy(\"Region\").orderBy(col(\"Sales\").desc())\n",
        "\n",
        "df=df.withColumn(\"cumulative\",sum(\"Sales\").over(window_cum))\\\n",
        ".withColumn(\"rank\",rank().over(window_spec))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnVjzVG3j279",
        "outputId": "171ae13a-19c1-4301-ff32-8b81b4b2fb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+----------+----+\n",
            "|Region|Month|Sales|cumulative|rank|\n",
            "+------+-----+-----+----------+----+\n",
            "|  East|  Feb|  200|       200|   1|\n",
            "|  East|  Jan|  200|       400|   1|\n",
            "|  East|  Mar|  150|       550|   3|\n",
            "|  West|  Mar|  450|      1200|   1|\n",
            "|  West|  Jan|  400|       750|   2|\n",
            "|  West|  Feb|  350|       350|   3|\n",
            "+------+-----+-----+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7)You are given a large e-commerce transaction dataset stored in a partitioned format based on country. Your task is to count the distinct number of products purchased (product_id) for each customer_id in every country. The result should include the country, customer ID, and the distinct product count."
      ],
      "metadata": {
        "id": "cyt7ASjakMFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "data = [ (\"USA\", 101, \"P001\"), (\"USA\", 101, \"P002\"), (\"USA\", 101, \"P001\"), (\"USA\", 102, \"P003\"), (\"USA\", 102, \"P003\"), (\"UK\", 201, \"P004\"), (\"UK\", 201, \"P005\"), (\"UK\", 202, \"P004\"), (\"UK\", 202, \"P005\"), (\"UK\", 202, \"P004\") ]\n",
        "\n",
        "columns = [\"country\", \"customer_id\", \"product_id\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O16RmJ_GkzFG",
        "outputId": "edca62b2-6949-4edd-a712-8f87a7b3384b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+----------+\n",
            "|country|customer_id|product_id|\n",
            "+-------+-----------+----------+\n",
            "|    USA|        101|      P001|\n",
            "|    USA|        101|      P002|\n",
            "|    USA|        101|      P001|\n",
            "|    USA|        102|      P003|\n",
            "|    USA|        102|      P003|\n",
            "|     UK|        201|      P004|\n",
            "|     UK|        201|      P005|\n",
            "|     UK|        202|      P004|\n",
            "|     UK|        202|      P005|\n",
            "|     UK|        202|      P004|\n",
            "+-------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df=df.groupBy(\"country\",\"customer_id\").agg(countDistinct(\"product_id\").alias(\"distincr_product\")).orderBy(\"customer_id\")\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoH_ND_YlOgj",
        "outputId": "41ac4f59-9832-4a00-bea5-feea0b4c78d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+----------------+\n",
            "|country|customer_id|distincr_product|\n",
            "+-------+-----------+----------------+\n",
            "|    USA|        101|               2|\n",
            "|    USA|        102|               1|\n",
            "|     UK|        201|               2|\n",
            "|     UK|        202|               2|\n",
            "+-------+-----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8)"
      ],
      "metadata": {
        "id": "jJrEEf3QoYzq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fweVRwL6oaLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9)Write a PySpark program to calculate the highest salary in each department and sort the output in ascending order of department name."
      ],
      "metadata": {
        "id": "ROQLD9tyoapB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import *\n",
        "data = [(1, \"Ravi\", \"IT\", 85000),(2, \"Priya\", \"HR\", 65000),(3, \"Suresh\", \"IT\", 92000),(4, \"Neha\", \"Finance\", 78000),\n",
        "(5, \"Anil\", \"HR\", 72000),(6, \"Divya\", \"Finance\", 88000),(7, \"Kiran\", \"IT\", 88000)]\n",
        "\n",
        "columns = [\"emp_id\", \"emp_name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V5d8VDLos0J",
        "outputId": "496f3261-ad89-4a92-86cd-c7a852f389b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+------+\n",
            "|emp_id|emp_name|department|salary|\n",
            "+------+--------+----------+------+\n",
            "|     1|    Ravi|        IT| 85000|\n",
            "|     2|   Priya|        HR| 65000|\n",
            "|     3|  Suresh|        IT| 92000|\n",
            "|     4|    Neha|   Finance| 78000|\n",
            "|     5|    Anil|        HR| 72000|\n",
            "|     6|   Divya|   Finance| 88000|\n",
            "|     7|   Kiran|        IT| 88000|\n",
            "+------+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec=Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
        "rank_df = df.withColumn(\"rank\",dense_rank().over(window_spec))\n",
        "rank1=rank_df.filter(col(\"rank\")==1).orderBy(\"department\")\n",
        "rank1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvK6zGzlo2LI",
        "outputId": "0e012d01-2d32-4899-a3ea-7a06d6af6599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+------+----+\n",
            "|emp_id|emp_name|department|salary|rank|\n",
            "+------+--------+----------+------+----+\n",
            "|     6|   Divya|   Finance| 88000|   1|\n",
            "|     5|    Anil|        HR| 72000|   1|\n",
            "|     3|  Suresh|        IT| 92000|   1|\n",
            "+------+--------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10)You are working as a Data Engineer at a retail company. The marketing team has provided a dataset of customer purchases to analyze the relationship between the amount spent on advertisements and the revenue generated. Using PySpark, compute the correlation between the \"Ad_Spend\" and \"Revenue\" columns to determine if there's a linear relationship."
      ],
      "metadata": {
        "id": "MxbhZQFP_KF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "schema = StructType([ StructField(\"Customer_ID\", StringType(), True), StructField(\"Ad_Spend\", IntegerType(), True), StructField(\"Revenue\", IntegerType(), True) ])\n",
        "\n",
        "data = [ (\"C001\", 2000, 25000), (\"C002\", 1500, 23000), (\"C003\", 3000, 40000), (\"C004\", 1200, 18000), (\"C005\", 2500, 30000) ]\n",
        "\n",
        "df = spark.createDataFrame(data,schema)\n",
        "display(df)\n",
        "\n",
        "correl = df.stat.corr(\"Ad_Spend\",\"Revenue\")\n",
        "display(correl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "oKKTrWzA_RAj",
        "outputId": "01cfc520-0828-4d97-ed78-5f7ac284f53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Customer_ID: string, Ad_Spend: int, Revenue: int]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.9704535552410215"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are given a dataset containing sales data for different stores across various months. Each row contains the store name, the month, and the sales amount. Your task is to calculate the cumulative sales for each store, considering the monthly sales, using PySpark.\n",
        "\n",
        "You should also:\n",
        "Filter out stores with sales lower than 1000 in any month.\n",
        "Calculate the total sales for each store over all months.\n",
        "Sort the results by the total sales in descending order."
      ],
      "metadata": {
        "id": "FagYkNUl3RWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "data = [ (\"Store A\", \"2024-01\", 800), (\"Store A\", \"2024-02\", 1200), (\"Store A\", \"2024-03\", 900), (\"Store B\", \"2024-01\", 1500), (\"Store B\", \"2024-02\", 1600),\n",
        "        (\"Store B\", \"2024-03\", 1400), (\"Store C\", \"2024-01\", 700), (\"Store C\", \"2024-02\", 1000), (\"Store C\", \"2024-03\", 800) ]\n",
        "\n",
        "df=spark.createDataFrame(data,[\"Store\", \"Month\", \"Sales\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLHLTlqo2mdo",
        "outputId": "126e7a36-6e9f-4aa8-f462-0e98429bcaea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+\n",
            "|  Store|  Month|Sales|\n",
            "+-------+-------+-----+\n",
            "|Store A|2024-01|  800|\n",
            "|Store A|2024-02| 1200|\n",
            "|Store A|2024-03|  900|\n",
            "|Store B|2024-01| 1500|\n",
            "|Store B|2024-02| 1600|\n",
            "|Store B|2024-03| 1400|\n",
            "|Store C|2024-01|  700|\n",
            "|Store C|2024-02| 1000|\n",
            "|Store C|2024-03|  800|\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.Sales < 1000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zCdTEl93TSG",
        "outputId": "d4ac9ec0-f8d7-4c3a-c561-2fbae8e5f8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+\n",
            "|  Store|  Month|Sales|\n",
            "+-------+-------+-----+\n",
            "|Store A|2024-01|  800|\n",
            "|Store A|2024-03|  900|\n",
            "|Store C|2024-01|  700|\n",
            "|Store C|2024-03|  800|\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the total sales for each store over all months.\n",
        "window"
      ],
      "metadata": {
        "id": "rcmETeo64jZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1)Problem Statement:\n",
        "\n",
        "You are given a dataset that contains user activity logs, where each row represents a date a user logged into an app.\n",
        "Write a PySpark program to identify gaps in login activity per user — i.e., the date ranges when the user was inactive.\n",
        "\n",
        "✅ Input Columns:\n",
        "user_id: String (e.g., \"U001\")\n",
        "login_date: Date (e.g., \"2024-01-01\")\n",
        "\n",
        "Assume:\n",
        "Dates are not continuous.\n",
        "Each user is expected to be active daily.\n",
        "\n",
        "✅ Expected Output Columns:\n",
        "user_id\n",
        "inactive_from_date\n",
        "inactive_to_date\n",
        "Only output gaps of more than 1 day."
      ],
      "metadata": {
        "id": "WjF2up9r54lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col, lag, lead, to_date, datediff\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "data = [\n",
        "    (\"U001\", \"2024-01-01\"),\n",
        "    (\"U001\", \"2024-01-02\"),\n",
        "    (\"U001\", \"2024-01-05\"),\n",
        "    (\"U001\", \"2024-01-07\"),\n",
        "    (\"U002\", \"2024-01-03\"),\n",
        "    (\"U002\", \"2024-01-07\"),\n",
        "]\n",
        "column = [\"user_id\",\"login_date\"]\n",
        "df=spark.createDataFrame(data,column)\n",
        "df=df.withColumn(\"login_date\",col(\"login_date\").cast(\"date\"))\n",
        "window_spec=Window.partitionBy(\"user_id\").orderBy(\"login_date\")\n",
        "df=df.withColumn(\"previous_date\",lag(\"login_date\",1).over(window_spec))\n",
        "df=df.withColumn(\"gap\",datediff(\"login_date\",\"previous_date\"))\n",
        "df=df.filter(col(\"gap\")>1)\n",
        "result=df.select(\"user_id\",col(\"previous_date\").alias(\"inactive_from_date\"),col(\"login_date\").alias(\"inactive_to_date\"))\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjGq3h4q59JZ",
        "outputId": "c860aedd-3284-41cb-d028-e2067a3e038f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+----------------+\n",
            "|user_id|inactive_from_date|inactive_to_date|\n",
            "+-------+------------------+----------------+\n",
            "|   U001|        2024-01-02|      2024-01-05|\n",
            "|   U001|        2024-01-05|      2024-01-07|\n",
            "|   U002|        2024-01-03|      2024-01-07|\n",
            "+-------+------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario:\n",
        "You’re working as a Data Engineer for a power distribution company in India. Customers are billed daily, but due to technical issues, some records are missing in the billing_logs table. Management wants to find out the missing billing dates for each customer.\n",
        "\n",
        "Identify continuous date gaps in billing logs for each customer between their first and last billing date.\n",
        "\n",
        "data = [\n",
        " (\"C001\", \"2024-01-01\"),\n",
        " (\"C001\", \"2024-01-02\"),\n",
        " (\"C001\", \"2024-01-04\"),\n",
        " (\"C001\", \"2024-01-06\"),\n",
        " (\"C002\", \"2024-01-03\"),\n",
        " (\"C002\", \"2024-01-05\"),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"customer_id\", \"billing_date\"])\n",
        "\n",
        "\n",
        "Output :\n",
        "+------------+------------+------------+\n",
        "|customer_id |missing_from|missing_to |\n",
        "+------------+------------+------------+\n",
        "|C001 |2024-01-03 |2024-01-03 |\n",
        "|C001 |2024-01-05 |2024-01-05 |\n",
        "|C002 |2024-01-04 |2024-01-04 |\n",
        "+------------+------------+------------+\n"
      ],
      "metadata": {
        "id": "K_BhKHOJBhkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col, min, datediff\n",
        "data = [\n",
        " (\"U001\", \"2024-01-01\"),\n",
        " (\"U001\", \"2024-01-02\"), # Day 1\n",
        " (\"U001\", \"2024-01-08\"), # Day 7\n",
        " (\"U001\", \"2024-01-31\"), # Day 30\n",
        " (\"U002\", \"2024-01-01\"),\n",
        " (\"U002\", \"2024-01-03\"),\n",
        " (\"U003\", \"2024-01-02\"),\n",
        " (\"U003\", \"2024-01-03\"), # Day 1\n",
        " (\"U003\", \"2024-02-01\"), # Day 30\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"user_id\", \"login_date\"])\n",
        "df=df.withColumn(\"login_date\",col(\"login_date\").cast(\"date\"))\n",
        "signup_df = df.groupBy(\"user_id\").agg(min(\"login_date\").alias(\"signup_date\"))\n",
        "df=df.join(signup_df, on=\"user_id\")\n",
        "\n"
      ],
      "metadata": {
        "id": "t8X3qKqlBlzW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}